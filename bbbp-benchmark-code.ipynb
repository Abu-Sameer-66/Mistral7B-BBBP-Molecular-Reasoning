{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installation of required libraries for LLM Fine-tuning\n!pip install --quiet deepchem rdkit transformers peft bitsandbytes accelerate\n\nimport os, gc, torch, pandas as pd, numpy as np, deepchem as dc\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom sklearn.metrics import roc_auc_score\nimport torch.nn.functional as F\n\n# System and memory management for stable training on T4 GPU\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\ntorch.cuda.empty_cache()\ngc.collect()\n\n# --- 1. DATASET PREPARATION & SANITIZATION ---\n# Loading the BBBP dataset from DeepChem's S3 bucket\nurl = \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\"\ndf_raw = pd.read_csv(url)\n\n# Sanitization: Filtering out 11 molecules with invalid Nitrogen valences (valence > 4) \n# to ensure chemical validity and prevent numerical instability.\ndf_clean = df_raw[df_raw['smiles'].apply(lambda x: Chem.MolFromSmiles(x) is not None)].copy()\n\n# Implementing Scaffold Split for rigorous evaluation on out-of-distribution structures\ndataset = dc.data.NumpyDataset(X=np.zeros(len(df_clean)), ids=df_clean['smiles'].values)\ntrain_idx, _, test_idx = dc.splits.ScaffoldSplitter().split(dataset)\n\ndef build_property_aware_dataset(df, indices, is_train=True):\n    subset = df.iloc[indices]\n    data = []\n    for _, row in subset.iterrows():\n        mol = Chem.MolFromSmiles(row['smiles'])\n        # Calculating TPSA and Molecular Weight as key drivers for BBB permeability\n        mw = Descriptors.MolWt(mol)\n        tpsa = Descriptors.TPSA(mol)\n        \n        # Scientific Prompting: Providing physical context to guide the model's latent reasoning\n        prompt = (f\"Analysis: Brain Barrier Penetration\\n\"\n                  f\"Descriptors: MW={mw:.1f}, TPSA={tpsa:.1f}\\n\"\n                  f\"SMILES: {row['smiles']}\\n\"\n                  f\"Permeable: {'Yes' if int(row['p_np']) == 1 else 'No'}\")\n        \n        # Addressing class imbalance: Oversampling the minority 'No' class (3x to 4x)\n        repeat = 4 if (is_train and int(row['p_np']) == 0) else 1\n        for _ in range(repeat):\n            data.append({'prompt': prompt, 'label': int(row['p_np'])})\n    return pd.DataFrame(data)\n\ndf_train = build_property_aware_dataset(df_clean, train_idx).sample(frac=1)\ndf_test = build_property_aware_dataset(df_clean, test_idx, is_train=False)\n\n# --- 2. MODEL ARCHITECTURE (Mistral-7B + QLoRA) ---\nMODEL_ID = \"mistralai/Mistral-7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\ntokenizer.pad_token = tokenizer.eos_token\n\n# 4-bit NF4 Quantization to optimize memory for 16GB VRAM\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=torch.float16\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map=\"auto\")\nbase_model.gradient_checkpointing_enable()\n\n# Using LoRA Rank 32 to capture complex chemical relationships in the BBBP task\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, r=32, lora_alpha=64, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\nmodel = get_peft_model(base_model, peft_config)\nmodel.enable_input_require_grads()\n\n# --- 3. TRAINING & VALIDATION LOOP ---\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\nid_yes = tokenizer.encode(\"Yes\", add_special_tokens=False)[-1]\nid_no = tokenizer.encode(\"No\", add_special_tokens=False)[-1]\n\ntotal_steps = 2500\ncurrent_step = 0\nbest_auc = 0\ntrain_ds = dc.data.NumpyDataset(X=df_train['prompt'].values)\n\nprint(\"[*] Starting step-based fine-tuning...\")\nwhile current_step < total_steps:\n    for (X_batch, _, _, _) in train_ds.iterbatches(batch_size=2):\n        model.train()\n        tokens = tokenizer(X_batch.tolist(), padding=True, truncation=True, max_length=320, return_tensors=\"pt\").to(\"cuda\")\n        \n        loss = model(**tokens, labels=tokens[\"input_ids\"]).loss\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # Periodic validation every 250 steps to monitor generalization on scaffold split\n        if current_step % 250 == 0 and current_step > 0:\n            model.eval()\n            y_probs, y_true = [], []\n            with torch.no_grad():\n                for _, row in df_test.iterrows():\n                    eval_p = row['prompt'].split(\"Permeable:\")[0] + \"Permeable:\"\n                    inputs = tokenizer(eval_p, return_tensors='pt').to(\"cuda\")\n                    logits = model(input_ids=inputs['input_ids']).logits[0, -1, [id_no, id_yes]]\n                    y_probs.append(F.softmax(logits, dim=-1)[1].item())\n                    y_true.append(row['label'])\n            \n            auc = roc_auc_score(y_true, y_probs)\n            print(f\"Step {current_step} | Loss: {loss.item():.4f} | Validation ROC-AUC: {auc:.4f}\")\n            \n            if auc > best_auc:\n                best_auc = auc\n                model.save_pretrained(\"./best_bbbp_model\")\n        \n        current_step += 1\n        if current_step >= total_steps: break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}